{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "FILE_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.dcm', '.DCM', '.raw', '.RAW', '.svs', '.SVS']\n",
    "IMG_EXTENSION = ['.png', '.PNG', '.jpg', '.JPG', '.jpeg', '.JPEG']\n",
    "DCM_EXTENSION = ['.dcm', '.DCM']\n",
    "RAW_EXTENSION = ['.raw', '.RAW']\n",
    "NIFTI_EXTENSION = ['.nii']\n",
    "NP_EXTENSION = ['.npy']\n",
    "\n",
    "common_dir = '/home/ncp/workspace/202002n050/050.신경계 질환 관련 임상 및 진료 데이터'\n",
    "\n",
    "\n",
    "def check_extension(filename, extension_ls=FILE_EXTENSION):\n",
    "    return any(filename.endswith(extension) for extension in extension_ls)\n",
    "\n",
    "\n",
    "def load_file_path(folder_path, extension_ls=FILE_EXTENSION, all_sub_folders=False):\n",
    "    \"\"\"find 'IMG_EXTENSION' file paths in folder.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str) -- folder directory\n",
    "        extension_ls (list) -- list of extensions\n",
    "    \n",
    "    Return:\n",
    "        file_paths (list) -- list of 'extension_ls' file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    file_paths = []\n",
    "    assert os.path.isdir(folder_path), f'{folder_path} is not a valid directory'\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(folder_path)):\n",
    "        for fname in fnames:\n",
    "            if check_extension(fname, extension_ls):\n",
    "                path = os.path.join(root, fname)\n",
    "                file_paths.append(path)\n",
    "        if not all_sub_folders:\n",
    "            break\n",
    "\n",
    "    return file_paths[:]\n",
    "\n",
    "\n",
    "def gen_new_dir(new_dir):\n",
    "    try: \n",
    "        if not os.path.exists(new_dir): \n",
    "            os.makedirs(new_dir) \n",
    "            #print(f\"New directory!: {new_dir}\")\n",
    "    except OSError: \n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "        \n",
    "def get_data_fname_label_in_split(data_df, fold=0, mode='train'):\n",
    "    #return data_df[data_df['split_811_new']==mode][['name', 'bad_outcome_3m']].values\n",
    "    return data_df[data_df['fold_'+str(fold)]==mode][['name', 'bad_outcome_3m']].values\n",
    "\n",
    "def get_dataset(data_df, data_dir, mask_dir, fold=0, mode='train'):\n",
    "    data_fname_label_arr = get_data_fname_label_in_split(data_df, fold, mode=mode)\n",
    "    dwi_path_ls = sorted(load_file_path(os.path.join(data_dir, 'dwi'), NP_EXTENSION))\n",
    "    adc_path_ls = sorted(load_file_path(os.path.join(data_dir, 'adc'), NP_EXTENSION))\n",
    "    np_mask_path_ls = sorted(load_file_path(mask_dir, NP_EXTENSION))\n",
    "    dwi_path_dict = {os.path.splitext(os.path.basename(p))[0]:p for p in dwi_path_ls}\n",
    "    adc_path_dict = {os.path.splitext(os.path.basename(p))[0]:p for p in adc_path_ls}\n",
    "    np_mask_path_dict = {os.path.splitext(os.path.basename(p))[0]:p for p in np_mask_path_ls}\n",
    "    return [[dwi_path_dict.get(fname), adc_path_dict.get(fname), np_mask_path_dict.get(fname), label] \n",
    "            for fname, label in data_fname_label_arr if np_mask_path_dict.get(fname) if adc_path_dict.get(fname) if dwi_path_dict.get(fname)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img_3d):\n",
    "    img_3d = img_3d - img_3d.min()\n",
    "    if img_3d.max() != 0:\n",
    "        img_3d = img_3d / img_3d.max()\n",
    "    #img_3d = (img_3d-img_3d.min()) / (img_3d.max()-img_3d.min())\n",
    "    return img_3d.astype(np.float32)\n",
    "\n",
    "def z_normalize(img_3d):\n",
    "    return (img_3d - 0.5) / .5\n",
    "\n",
    "def img_loader(dwipath, adcpath, maskpath):\n",
    "    dwiimg = normalize(np.load(dwipath))\n",
    "    adcimg = normalize(np.load(adcpath))\n",
    "    #maskimg = normalize(np.load(maskpath))\n",
    "    return z_normalize(np.stack([dwiimg, adcimg], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('/home/ncp/workspace/blocks1/aihub_df_v.1.2.csv')\n",
    "data_df_da = data_df[data_df.dwi_adc == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_fname_label = data_df_da[['name', 'bad_outcome_3m']].values\n",
    "tot_fname = tot_fname_label[:,0]\n",
    "tot_label = tot_fname_label[:,1]\n",
    "train_fname, test_fname, train_label, test_label = train_test_split(tot_fname, \n",
    "                                                                    tot_label, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=17, \n",
    "                                                                    stratify=tot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tot = train_fname\n",
    "Y_tot = train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=8, random_state=76, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf.get_n_splits(X_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_ = []\n",
    "for train_idx, val_idx in kf.split(X_tot):\n",
    "    X_train, X_val = X_tot[train_idx], X_tot[val_idx]\n",
    "    fold_.append([X_train, X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(fname, train_fname, val_fname, test_fname):\n",
    "    if fname in train_fname:\n",
    "        return 'train'\n",
    "    elif fname in val_fname:\n",
    "        return 'val'\n",
    "    elif fname in test_fname:\n",
    "        return 'test'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, X_tv in enumerate(fold_):\n",
    "    X_t, X_v = X_tv\n",
    "    fold_n = 'fold_' + str(idx)\n",
    "    data_df_da[fold_n] = data_df_da['name'].map(lambda x: split_train_val_test(x, X_t, X_v, test_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_da.to_csv('/home/ncp/workspace/blocks1/aihub_df_v.KF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class AIHUB_GoodOutcomePredDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset_dir,\n",
    "                 mask_dir,\n",
    "                 dataset_df, \n",
    "                 img_loader=img_loader, \n",
    "                 fold = 0,\n",
    "                 mode='train'\n",
    "    ):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.dataset_df = pd.read_csv(dataset_df)\n",
    "        self.img_loader = img_loader\n",
    "        self.fold = fold\n",
    "        self.mode = mode\n",
    "        self.dataset = get_dataset(self.dataset_df, self.dataset_dir, self.mask_dir, self.fold, self.mode)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        dwi_path, adc_path, mask_path, label = self.dataset[index]\n",
    "        \n",
    "        image = img_loader(dwi_path, adc_path, mask_path)\n",
    "\n",
    "        return torch.Tensor(image), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, \n",
    "                 patience=7, \n",
    "                 verbose=False, \n",
    "                 delta=0, \n",
    "                 path='checkpoint.pt', \n",
    "                 trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased {self.val_loss_min:.6f} --> {val_loss:.6f}. Saving Model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1-alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1),-1)\n",
    "            input = input.transpose(1,2)\n",
    "            input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.view(-1,1)\n",
    "        \n",
    "        logpt = F.log_softmax(input, -1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "        \n",
    "        loss = -1*(1-pt)**self.gamma*logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(alpha=0.25, gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=100):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error = AverageMeter()\n",
    "    \n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    end = time.time()\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "        if torch.cuda.is_available():\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = model(input)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        #loss = criterion(output, target)\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.data.cpu().topk(1, dim=1)\n",
    "        error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if batch_idx % print_freq == 0:\n",
    "            res = '\\t'.join([\n",
    "                'Epoch: [%d/%d]' % (epoch+1, n_epochs),\n",
    "                'Iter: [%d/%d]' % (batch_idx+1, len(loader)),\n",
    "                'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
    "                'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
    "                'Error %.4f (%.4f)' % (error.val, error.avg),\n",
    "            ])\n",
    "            print(res)\n",
    "        \n",
    "    return batch_time.avg, losses.avg, error.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, loader, print_freq=5, is_test=True):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error = AverageMeter()\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "        \n",
    "            output = model(input)\n",
    "            loss = torch.nn.functional.cross_entropy(output, target)\n",
    "            #loss = criterion(output, target)\n",
    "        \n",
    "            batch_size = target.size(0)\n",
    "            _, pred = output.data.cpu().topk(1, dim=1)\n",
    "            error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            losses.update(loss.item(), batch_size)\n",
    "        \n",
    "            \n",
    "        \n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "        \n",
    "            if batch_idx % print_freq == 0:\n",
    "                res = '\\t'.join([\n",
    "                    'Test:' if is_test else 'Valid',\n",
    "                    'Iter: [%d/%d]' % (batch_idx+1, len(loader)),\n",
    "                    'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
    "                    'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
    "                    'Error %.4f (%.4f)' % (error.val, error.avg),\n",
    "                ])\n",
    "                print(res)\n",
    "        \n",
    "        return batch_time.avg, losses.avg, error.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, valid_set, test_set, save, n_epochs=300,\n",
    "         batch_size=64, lr=0.0001, patience=10, save_epoch=10, seed=None):\n",
    "    cnt=0\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                              batch_size=batch_size, drop_last=True, shuffle=True,\n",
    "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                              batch_size=batch_size, shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    if valid_set is None:\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_set,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    model_wrapper = model\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5*n_epochs, 0.75*n_epochs],\n",
    "                                                    gamma = 0.1)\n",
    "    \n",
    "    with open(os.path.join(save, 'results.csv'), 'w') as f:\n",
    "        f.write('epoch,train_loss,train_error,valid_loss,valid_error,test_error\\n')\n",
    "    \n",
    "    best_error = 1\n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss, train_error = train_epoch(\n",
    "            model=model_wrapper, \n",
    "            loader=train_loader, \n",
    "            optimizer=optimizer, \n",
    "            epoch=epoch, \n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "        scheduler.step()\n",
    "        _, valid_loss, valid_error = test_epoch(\n",
    "            model=model_wrapper, \n",
    "            loader=valid_loader if valid_loader else test_loader, \n",
    "            is_test=(not valid_loader)\n",
    "        )\n",
    "        \n",
    "        if valid_loader:\n",
    "            if valid_error < best_error:\n",
    "                best_error = valid_error\n",
    "                print('New best error: %.4f' % best_error)\n",
    "                torch.save(model.state_dict(), os.path.join(save, 'model_best.dat'))\n",
    "                torch.save(model.state_dict(), os.path.join(save, 'model_epoch'+str(cnt).zfill(3)+'.dat'))\n",
    "        else:\n",
    "            if (cnt%save_epoch==0):\n",
    "                #torch.save(model.state_dict(), os.path.join(save, 'model_epoch'+str(cnt).zfill(3)+'dat'))\n",
    "                pass\n",
    "        \n",
    "        with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
    "            f.write('%04d,%0.6f,%0.6f,%0.5f,%0.5f,\\n' % (\n",
    "                (epoch+1), \n",
    "                train_loss, \n",
    "                train_error, \n",
    "                valid_loss, \n",
    "                valid_error\n",
    "            ))\n",
    "        cnt += 1\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(save, 'model_final.dat'))\n",
    "    \n",
    "    model.load_state_dict(torch.load(os.path.join(save, 'model_final.dat')))\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    test_results = test_epoch(\n",
    "        model=model, \n",
    "        loader=test_loader, \n",
    "        is_test=True\n",
    "    )\n",
    "    \n",
    "    _, _, test_error = test_results\n",
    "    with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
    "        f.write(',,,,,%0.5f\\n' % (test_error))\n",
    "    print('Final test error: %.4f' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(save, \n",
    "         model, \n",
    "         n_epochs=300, \n",
    "         batch_size=64, \n",
    "         lr=0.0001, \n",
    "         patience=10, \n",
    "         fold=0,\n",
    "         seed=None):\n",
    "    train_dataset = AIHUB_GoodOutcomePredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/dicom_to_np_2dnorm_resample',\n",
    "        mask_dir='/home/ncp/workspace/blocks1/refined_mask_resample',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/aihub_df_v.KF.csv',\n",
    "        fold = fold,\n",
    "        mode='train')\n",
    "    val_dataset = AIHUB_GoodOutcomePredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/dicom_to_np_2dnorm_resample',\n",
    "        mask_dir='/home/ncp/workspace/blocks1/refined_mask_resample',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/aihub_df_v.KF.csv', \n",
    "        fold = fold,\n",
    "        mode='val')\n",
    "    test_dataset = AIHUB_GoodOutcomePredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/dicom_to_np_2dnorm_resample',\n",
    "        mask_dir='/home/ncp/workspace/blocks1/refined_mask_resample',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/aihub_df_v.KF.csv', \n",
    "        fold = fold,\n",
    "        mode='test')\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print('Total parameters: ', num_params)\n",
    "    \n",
    "    if not os.path.exists(save):\n",
    "        os.makedirs(save)\n",
    "    if not os.path.isdir(save):\n",
    "        raise Exception('%s is not a dir' % save)\n",
    "    \n",
    "    train(model=model, train_set=train_dataset, valid_set=val_dataset, test_set=test_dataset, save=save, n_epochs=n_epochs,\n",
    "         batch_size=batch_size, lr=lr, patience=patience, seed=seed)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import resnet, wide_resnet, resnext, densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './DWIADC/KFold/Fold3/3DDenseNet169d_new'\n",
    "gen_new_dir(save_path)\n",
    "N_EPOCHS = 10000\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.00001\n",
    "PATIENCE = 10\n",
    "\n",
    "model = densenet.densenet169(\n",
    "                num_classes=2,\n",
    "                spatial_size=256,\n",
    "                sample_duration=20)\n",
    "\n",
    "model.features.conv0 = torch.nn.Conv3d(2, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo(save=save_path, \n",
    "     model=model, \n",
    "     n_epochs=N_EPOCHS, \n",
    "     batch_size=BATCH_SIZE, \n",
    "     lr=LR, \n",
    "     patience=PATIENCE, \n",
    "     fold = 3,\n",
    "     seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(testloader, model, threshold=0.5):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    output_arr = np.ones((1,2))\n",
    "    label_arr = np.array([])\n",
    "    pred_arr = np.array([])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            output_arr = np.concatenate((output_arr, outputs.softmax(1).cpu().numpy()), axis=0)\n",
    "            label_arr = np.concatenate((label_arr, labels.cpu().numpy()), axis=0)\n",
    "            pred_arr = np.concatenate((pred_arr, predicted.cpu().numpy()), axis=0)\n",
    "            \n",
    "        output_arr = np.delete(output_arr, 0, axis=0)\n",
    "        acc = correct / total\n",
    "        print('Accuracy on the test images: ', (100*correct/total))\n",
    "        return acc, output_arr, label_arr, pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 3\n",
    "save_path = './DWIADC/KFold/Fold3/3DDenseNet169d_new'\n",
    "test_model = densenet.densenet169(\n",
    "                num_classes=2,\n",
    "                spatial_size=256,\n",
    "                sample_duration=18)\n",
    "\n",
    "test_model.features.conv0 = torch.nn.Conv3d(2, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "#model.features.norm5 = torch.nn.BatchNorm3d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#test_model.classifier = torch.nn.Linear(in_features=7680, out_features=2, bias=True)\n",
    "test_model.load_state_dict(torch.load(os.path.join(save_path, 'model_best.dat')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AIHUB_GoodOutcomePredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/dicom_to_np_2dnorm_resample',\n",
    "        mask_dir='/home/ncp/workspace/blocks1/refined_mask_resample',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/aihub_df_v.KF.csv',\n",
    "        fold = fold, \n",
    "        mode='train')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=8, shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_output_arr, train_label_arr, train_pred_arr = test_acc(test_loader, test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(train_label_arr==1, train_output_arr[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = metrics.roc_curve(train_label_arr==i, train_output_arr[:,i])\n",
    "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "roc_auc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr[0], tpr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df = pd.read_csv('/home/ncp/workspace/AIHUB_dataset/df_csv_merged_v2.1.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lesion_area_df = pd.read_csv('/home/ncp/workspace/blocks2/pred_lesion_area_df_og.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_info_arr = []\n",
    "lesion_info_arr = []\n",
    "label_arr = []\n",
    "for f_path, _, _, label in test_dataset.dataset:\n",
    "    fname = os.path.splitext(os.path.basename(f_path))[0]\n",
    "    tabular_info = tabular_df[tabular_df.name == fname][['pre_good_mrs', 'age_cate', 'ini_nih', 'END']].values\n",
    "    lesion_area_info = pred_lesion_area_df[pred_lesion_area_df.name == fname].pred_lesion_area.values[0] #*100000\n",
    "    tabular_info_arr.append(tabular_info)\n",
    "    lesion_info_arr.append(lesion_area_info)\n",
    "    label_arr.append(label)\n",
    "tabular_info_arr = np.array(tabular_info_arr)\n",
    "lesion_info_arr = np.array(lesion_info_arr)\n",
    "label_arr = np.array(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([np.squeeze(tabular_info_arr), train_output_arr[:,1][:,np.newaxis], lesion_info_arr[:,np.newaxis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_df = pd.DataFrame(np.hstack([X_train, Y_train[:,np.newaxis]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_df.dropna(axis=0, inplace=True, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=17)\n",
    "model.fit(XY_train_df.iloc[:,:6], XY_train_df.iloc[:,6])#XY_train_df.iloc[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'ensemble3.pkl'), 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(XY_train_df.iloc[:,:7], XY_train_df.iloc[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = AIHUB_GoodOutcomePredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/dicom_to_np_2dnorm_resample',\n",
    "        mask_dir='/home/ncp/workspace/blocks1/refined_mask_resample',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/aihub_df_v.KF.csv',\n",
    "        fold = fold, \n",
    "        mode='val')\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                              batch_size=8, shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc, val_output_arr, val_label_arr, val_pred_arr = test_acc(val_loader, test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_info_arr = []\n",
    "lesion_info_arr = []\n",
    "label_arr = []\n",
    "for f_path, _, _, label in val_dataset.dataset:\n",
    "    fname = os.path.splitext(os.path.basename(f_path))[0]\n",
    "    tabular_info = tabular_df[tabular_df.name == fname][['pre_good_mrs', 'age_cate', 'ini_nih', 'END']].values\n",
    "    lesion_area_info = pred_lesion_area_df[pred_lesion_area_df.name == fname].pred_lesion_area.values[0] #*100000\n",
    "    tabular_info_arr.append(tabular_info)\n",
    "    lesion_info_arr.append(lesion_area_info)\n",
    "    label_arr.append(label)\n",
    "tabular_info_arr = np.array(tabular_info_arr)\n",
    "lesion_info_arr = np.array(lesion_info_arr)\n",
    "label_arr = np.array(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.hstack([np.squeeze(tabular_info_arr), val_output_arr[:,1][:,np.newaxis], lesion_info_arr[:,np.newaxis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val = val_label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_val_df = pd.DataFrame(np.hstack([X_val, Y_val[:,np.newaxis]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_val_df.dropna(axis=0, inplace=True, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldlvol_val_out_proba = model.predict_proba(XY_val_df.iloc[:,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label = XY_val_df.iloc[:,6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_label==1, cldlvol_val_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(val_label_arr==1, val_output_arr[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AIHUB_GoodOutcomePredDataset(\n",
    "        dataset_dir='/home/ncp/workspace/blocks1/dicom_to_np_2dnorm_resample',\n",
    "        mask_dir='/home/ncp/workspace/blocks1/refined_mask_resample',\n",
    "        dataset_df='/home/ncp/workspace/blocks1/aihub_df_v.KF.csv',\n",
    "        fold = fold, \n",
    "        mode='test')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=8, shuffle=False,\n",
    "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_acc, test_output_arr, test_label_arr, test_pred_arr = test_acc(test_loader, test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_info_arr = []\n",
    "lesion_info_arr = []\n",
    "label_arr = []\n",
    "for f_path, _, _, label in test_dataset.dataset:\n",
    "    fname = os.path.splitext(os.path.basename(f_path))[0]\n",
    "    tabular_info = tabular_df[tabular_df.name == fname][['pre_good_mrs', 'age_cate', 'ini_nih', 'END']].values\n",
    "    lesion_area_info = pred_lesion_area_df[pred_lesion_area_df.name == fname].pred_lesion_area.values[0] #*10000\n",
    "    tabular_info_arr.append(tabular_info)\n",
    "    lesion_info_arr.append(lesion_area_info)\n",
    "    label_arr.append(label)\n",
    "tabular_info_arr = np.array(tabular_info_arr)\n",
    "lesion_info_arr = np.array(lesion_info_arr)\n",
    "label_arr = np.array(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack([np.squeeze(tabular_info_arr), test_output_arr[:,1][:,np.newaxis], lesion_info_arr[:,np.newaxis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = test_label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_test_df = pd.DataFrame(np.hstack([X_test, Y_test[:,np.newaxis]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_test_df.dropna(axis=0, inplace=True, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldlvol_test_out_proba = model.predict_proba(XY_test_df.iloc[:,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label = XY_test_df.iloc[:,6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_label==1, cldlvol_test_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(test_label_arr==1, test_output_arr[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cd = RandomForestClassifier(n_estimators=500,max_depth=5, random_state=17)\n",
    "model_cd.fit(XY_train_df.iloc[:,:4], XY_train_df.iloc[:,6])#XY_train_df.iloc[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_val_out_proba = model_cd.predict_proba(XY_val_df.iloc[:,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(XY_val_df.iloc[:,6].values==1, cl_val_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_test_out_proba = model_cd.predict_proba(XY_test_df.iloc[:,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(XY_test_df.iloc[:,6].values==1, cl_test_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'clinical_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_cd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cd = RandomForestClassifier(n_estimators=500,max_depth=5, random_state=17)\n",
    "model_cd.fit(XY_train_df.iloc[:,:5], XY_train_df.iloc[:,6])#XY_train_df.iloc[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldl_val_out_proba = model_cd.predict_proba(XY_val_df.iloc[:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(XY_val_df.iloc[:,6].values==1, cldl_val_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldl_test_out_proba = model_cd.predict_proba(XY_test_df.iloc[:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(XY_test_df.iloc[:,6].values==1, cldl_test_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'ensemble1.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_cd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cd = RandomForestClassifier(n_estimators=500,max_depth=5, random_state=17)\n",
    "model_cd.fit(XY_train_df[[0,1,2,3,5]], XY_train_df.iloc[:,6])#XY_train_df.iloc[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_vol_val_out_proba = model_cd.predict_proba(XY_val_df[[0,1,2,3,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(XY_val_df.iloc[:,6].values==1, cl_vol_val_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_vol_test_out_proba = model_cd.predict_proba(XY_test_df[[0,1,2,3,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(XY_test_df.iloc[:,6].values==1, cl_vol_test_out_proba[:,1])\n",
    "J = tpr - fpr # Youden's J statistic\n",
    "idx = np.argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "sens, spec = tpr[idx], 1-fpr[idx]\n",
    "print(f\"ROCAUC:\\t\\t\\t\\t\\t{roc_auc}\")\n",
    "print(f\"Best threshold(Youden's J statistic):\\t{best_thresh}\")\n",
    "print(f\"Sensitivity:\\t\\t\\t\\t{sens}\")\n",
    "print(f\"Specificity:\\t\\t\\t\\t{spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'ensemble2.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_cd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_f_path = [os.path.splitext(os.path.basename(f_path))[0] for f_path, _, _, _ in val_dataset.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prob = np.stack([val_f_path, \n",
    "                     cl_val_out_proba[:,1], \n",
    "                     val_output_arr[:,1], \n",
    "                     cldl_val_out_proba[:,1], \n",
    "                     cl_vol_val_out_proba[:,1], \n",
    "                     cldlvol_val_out_proba[:,1], \n",
    "                     XY_val_df.iloc[:,6].values], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prob_df = pd.DataFrame(val_prob, \n",
    "                           columns=['name', \n",
    "                                    'clinical_data', \n",
    "                                    'image_DL', \n",
    "                                    'ensemble1', \n",
    "                                    'ensemble2', \n",
    "                                    'ensemble3', \n",
    "                                    'bad_outcome_3m'])\n",
    "val_prob_df.to_csv(os.path.join(save_path, 'val_prob.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f_path = [os.path.splitext(os.path.basename(f_path))[0] for f_path, _, _, _ in test_dataset.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob = np.stack([test_f_path, \n",
    "                     cl_test_out_proba[:,1], \n",
    "                     test_output_arr[:,1], \n",
    "                     cldl_test_out_proba[:,1], \n",
    "                     cl_vol_test_out_proba[:,1], \n",
    "                     cldlvol_test_out_proba[:,1], \n",
    "                     XY_test_df.iloc[:,6].values], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob_df = pd.DataFrame(test_prob, \n",
    "                           columns=['name', \n",
    "                                    'clinical_data', \n",
    "                                    'image_DL', \n",
    "                                    'ensemble1', \n",
    "                                    'ensemble2', \n",
    "                                    'ensemble3', \n",
    "                                    'bad_outcome_3m'])\n",
    "test_prob_df.to_csv(os.path.join(save_path, 'test_prob.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y))*sum([kernel(x,y) for x in X for y in Y])\n",
    "\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y<X)\n",
    "\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a, b in zip(V_A, V_B)])\n",
    "\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B) / ((var_A + var_B - 2*covar_AB)**(.5))\n",
    "\n",
    "def group_preds_by_label(preds, actual):\n",
    "    X = [p for (p,a) in zip(preds, actual) if a]\n",
    "    Y = [p for (p,a) in zip(preds, actual) if not a]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[test] A : clinical data / B : ensemble3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_A = np.array([.5,.5,.5,.5,.5,.5,.5,.5,.5,.5])\n",
    "preds_B = np.array([.2,.5,.1,.4,.9,.8,.7,.5,.9,.8])\n",
    "actual = np.array([0,0,0,0,1,0,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_A = cl_test_out_proba[:,1]\n",
    "preds_B = en3_test_out_proba[:,1]\n",
    "actual = XY_test_df.iloc[:,6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "\n",
    "V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "\n",
    "auc_A = auc(X_A, Y_A)\n",
    "auc_B = auc(X_B, Y_B)\n",
    "\n",
    "var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A)*1/len(V_A10)\n",
    "        + get_S_entry(V_A01, V_A01, auc_A, auc_A)*1/len(V_A01))\n",
    "\n",
    "var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B)*1/len(V_B10)\n",
    "        + get_S_entry(V_B01, V_B01, auc_B, auc_B)*1/len(V_B01))\n",
    "\n",
    "covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "           + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "z = z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "p = st.norm.sf(abs(z))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[test] A : DL / B : ensemble3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_A = test_output_arr[:,1]\n",
    "preds_B = en3_test_out_proba[:,1]\n",
    "actual = XY_test_df.iloc[:,6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "\n",
    "V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "\n",
    "auc_A = auc(X_A, Y_A)\n",
    "auc_B = auc(X_B, Y_B)\n",
    "\n",
    "var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A)*1/len(V_A10)\n",
    "        + get_S_entry(V_A01, V_A01, auc_A, auc_A)*1/len(V_A01))\n",
    "\n",
    "var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B)*1/len(V_B10)\n",
    "        + get_S_entry(V_B01, V_B01, auc_B, auc_B)*1/len(V_B01))\n",
    "\n",
    "covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "           + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "z = z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "p = st.norm.sf(abs(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.lin import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column=[]\n",
    "list_fi=[]\n",
    "for i,j in zip(XY_train_df.iloc[:,:7].columns,model.feature_importances_):\n",
    "    list_column.append(i)\n",
    "    list_fi.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance=pd.DataFrame(list_column,columns=['list_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance['list_fi']=list_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance.sort_values('list_fi',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
